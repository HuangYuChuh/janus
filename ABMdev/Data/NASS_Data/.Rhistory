filter(reference_period_desc == 'YEAR') %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(asd_desc, agg_level_desc, year, short_desc, class_desc, domain_desc, value_char =value_trim, unit_desc, commodity_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char)%>%
separate(short_desc, c("crop", 'info'), "- ")
}
ID<-sales(id_ops_raw_data)
my_year <- "1990"
###--------------------------------------#
# Download data and turn into dataframe
#####
# ID path string
path_id_ops <- paste0("api/api_GET/?key=", nass_key, "&sector_desc=", my_group_desc, "&year=", my_year, "&state_alpha=", "ID")
#unpack JSON object
raw_id_ops <- GET(url = nass_url, path = path_id_ops)
char_raw_id_ops<- rawToChar(raw_id_ops$content)
# check size of object
nchar(char_raw_id_ops)
#turn into list
list_raw_id_ops<- fromJSON(char_raw_id_ops)
# apply rbind to each row of the list and convert to a data frame
id_ops_raw_data <- pmap_dfr(list_raw_id_ops, rbind)
# US path string
path_us_ops <- paste0("api/api_GET/?key=", nass_key, "&sector_desc=", my_group_desc, "&year=", my_year, "&state_alpha=", "US")
#unpack JSON object
raw_us_ops <- GET(url = nass_url, path = path_us_ops)
char_raw_us_ops<- rawToChar(raw_us_ops$content)
# check size of object
nchar(char_raw_us_ops)
#turn into list
list_raw_us_ops<- fromJSON(char_raw_us_ops)
# apply rbind to each row of the list and convert to a data frame
us_ops_raw_data <- pmap_dfr(list_raw_us_ops, rbind)
###--------------------------------------#
# Subset Data based on highest value crops
#####
categories<-c("AREA HARVESTED", "PRICE RECEIVED", "YIELD")
agg_level<-c("STATE", "NATIONAL", "AGRICULTURAL DISTRICT")
ref_period<-c("YEAR", "MARKETING YEAR")
sales<- function(raw_data){
out <- raw_data %>%
#filter to specific data
filter(statisticcat_desc %in% categories)%>%
filter(agg_level_desc %in% agg_level) %>%
filter(reference_period_desc %in% ref_period) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(asd_desc,
agg_level_desc, year, short_desc, class_desc, domain_desc, value_char =value_trim, unit_desc, commodity_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char)%>%
separate(short_desc, c("crop", 'info'), "- ")
}
ID<-sales(id_ops_raw_data)
US<-sales(us_ops_raw_data)
#It would be ideal if there was an automated sorting and processing of this data to get at values, also -- we used CDL areas to get at the value for 2010 (e.g. crops that aren't going to be quantified bc of an individual farmer)
write.csv(ID, file='IdahoSales_1990.csv')
write.csv(US, file='NationalSales_1990.csv')
plot(id_sales$year[id_yeilds$crop == 'HOPS '], id_sales$value[id_sales$crop == 'HOPS '])
#
#Get price, value and hectares of top crops in Idaho
library(httr)
library(jsonlite)
library(tidycensus)
library(tidyverse)
library(purrr)
library(mapview)
library(dplyr)
# If you've never used your tidycensus API key in your R session, run this:
census_api_key("6fd2754dd1bdcc811b51c669667df2873b3bd56e")
nass_key <- "B5240598-2A7D-38EE-BF8D-816A27BEF504" #QuickStats
# NASS url
nass_url <- "http://quickstats.nass.usda.gov"
my_group_desc <-"CROPS"
# query start year 1975, 1990, 2005,
my_year <- "1990"
###--------------------------------------#
# Download data and turn into dataframe
#####
# ID path string
path_id_ops <- paste0("api/api_GET/?key=", nass_key, "&sector_desc=", my_group_desc, "&year=", my_year, "&state_alpha=", "ID")
#unpack JSON object
raw_id_ops <- GET(url = nass_url, path = path_id_ops)
char_raw_id_ops<- rawToChar(raw_id_ops$content)
# check size of object
nchar(char_raw_id_ops)
#turn into list
list_raw_id_ops<- fromJSON(char_raw_id_ops)
# apply rbind to each row of the list and convert to a data frame
id_ops_raw_data <- pmap_dfr(list_raw_id_ops, rbind)
# US path string
path_us_ops <- paste0("api/api_GET/?key=", nass_key, "&sector_desc=", my_group_desc, "&year=", my_year, "&state_alpha=", "US")
#unpack JSON object
raw_us_ops <- GET(url = nass_url, path = path_us_ops)
char_raw_us_ops<- rawToChar(raw_us_ops$content)
# check size of object
nchar(char_raw_us_ops)
#turn into list
list_raw_us_ops<- fromJSON(char_raw_us_ops)
# apply rbind to each row of the list and convert to a data frame
us_ops_raw_data <- pmap_dfr(list_raw_us_ops, rbind)
###--------------------------------------#
# Subset Data based on highest value crops
#####
categories<-c("AREA HARVESTED", "PRICE RECEIVED", "YIELD")
agg_level<-c("STATE", "NATIONAL", "AGRICULTURAL DISTRICT")
ref_period<-c("YEAR", "MARKETING YEAR")
sales<- function(raw_data){
out <- raw_data %>%
#filter to specific data
filter(statisticcat_desc %in% categories)%>%
filter(agg_level_desc %in% agg_level) %>%
filter(reference_period_desc %in% ref_period) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(asd_desc,
agg_level_desc, year, short_desc, class_desc, domain_desc, value_char =value_trim, unit_desc, commodity_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char)%>%
separate(short_desc, c("crop", 'info'), "- ")
}
ID<-sales(id_ops_raw_data)
US<-sales(us_ops_raw_data)
#It would be ideal if there was an automated sorting and processing of this data to get at values, also -- we used CDL areas to get at the value for 2010 (e.g. crops that aren't going to be quantified bc of an individual farmer)
write.csv(ID, file='IdahoSales_1990.csv')
write.csv(US, file='NationalSales_1990.csv')
setwd("~/Documents/GitRepos/IM3-BoiseState/ABMdev/Data/NASS_Data")
#Get price, value and hectares of top crops in Idaho
library(httr)
library(jsonlite)
library(tidycensus)
library(tidyverse)
library(purrr)
library(mapview)
library(dplyr)
# If you've never used your tidycensus API key in your R session, run this:
census_api_key("6fd2754dd1bdcc811b51c669667df2873b3bd56e")
nass_key <- "B5240598-2A7D-38EE-BF8D-816A27BEF504" #QuickStats
# NASS url
nass_url <- "http://quickstats.nass.usda.gov"
my_group_desc <-"CROPS"
# query start year 1975, 1990, 2005,
my_year <- "2005"
###--------------------------------------#
# Download data and turn into dataframe
#####
# ID path string
path_id_ops <- paste0("api/api_GET/?key=", nass_key, "&sector_desc=", my_group_desc, "&year=", my_year, "&state_alpha=", "ID")
#unpack JSON object
raw_id_ops <- GET(url = nass_url, path = path_id_ops)
char_raw_id_ops<- rawToChar(raw_id_ops$content)
# check size of object
nchar(char_raw_id_ops)
#turn into list
list_raw_id_ops<- fromJSON(char_raw_id_ops)
# apply rbind to each row of the list and convert to a data frame
id_ops_raw_data <- pmap_dfr(list_raw_id_ops, rbind)
# US path string
path_us_ops <- paste0("api/api_GET/?key=", nass_key, "&sector_desc=", my_group_desc, "&year=", my_year, "&state_alpha=", "US")
#unpack JSON object
raw_us_ops <- GET(url = nass_url, path = path_us_ops)
char_raw_us_ops<- rawToChar(raw_us_ops$content)
# check size of object
nchar(char_raw_us_ops)
#turn into list
list_raw_us_ops<- fromJSON(char_raw_us_ops)
# apply rbind to each row of the list and convert to a data frame
us_ops_raw_data <- pmap_dfr(list_raw_us_ops, rbind)
###--------------------------------------#
# Subset Data based on highest value crops
#####
categories<-c("AREA HARVESTED", "PRICE RECEIVED", "YIELD")
agg_level<-c("STATE", "NATIONAL", "AGRICULTURAL DISTRICT")
ref_period<-c("YEAR", "MARKETING YEAR")
sales<- function(raw_data){
out <- raw_data %>%
#filter to specific data
filter(statisticcat_desc %in% categories)%>%
filter(agg_level_desc %in% agg_level) %>%
filter(reference_period_desc %in% ref_period) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(asd_desc,
agg_level_desc, year, short_desc, class_desc, domain_desc, value_char =value_trim, unit_desc, commodity_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char)%>%
separate(short_desc, c("crop", 'info'), "- ")
}
ID<-sales(id_ops_raw_data)
US<-sales(us_ops_raw_data)
#It would be ideal if there was an automated sorting and processing of this data to get at values, also -- we used CDL areas to get at the value for 2010 (e.g. crops that aren't going to be quantified bc of an individual farmer)
write.csv(ID, file='IdahoSales_2005.csv')
write.csv(US, file='NationalSales_2005.csv')
# query start year 1975, 1990, 2005,
my_year <- "2010"
###--------------------------------------#
# Download data and turn into dataframe
#####
# ID path string
path_id_ops <- paste0("api/api_GET/?key=", nass_key, "&sector_desc=", my_group_desc, "&year=", my_year, "&state_alpha=", "ID")
#unpack JSON object
raw_id_ops <- GET(url = nass_url, path = path_id_ops)
char_raw_id_ops<- rawToChar(raw_id_ops$content)
# check size of object
nchar(char_raw_id_ops)
#turn into list
list_raw_id_ops<- fromJSON(char_raw_id_ops)
# apply rbind to each row of the list and convert to a data frame
id_ops_raw_data <- pmap_dfr(list_raw_id_ops, rbind)
# US path string
path_us_ops <- paste0("api/api_GET/?key=", nass_key, "&sector_desc=", my_group_desc, "&year=", my_year, "&state_alpha=", "US")
#unpack JSON object
raw_us_ops <- GET(url = nass_url, path = path_us_ops)
char_raw_us_ops<- rawToChar(raw_us_ops$content)
# check size of object
nchar(char_raw_us_ops)
#turn into list
list_raw_us_ops<- fromJSON(char_raw_us_ops)
# apply rbind to each row of the list and convert to a data frame
us_ops_raw_data <- pmap_dfr(list_raw_us_ops, rbind)
###--------------------------------------#
# Subset Data based on highest value crops
#####
categories<-c("AREA HARVESTED", "PRICE RECEIVED", "YIELD")
agg_level<-c("STATE", "NATIONAL", "AGRICULTURAL DISTRICT")
ref_period<-c("YEAR", "MARKETING YEAR")
sales<- function(raw_data){
out <- raw_data %>%
#filter to specific data
filter(statisticcat_desc %in% categories)%>%
filter(agg_level_desc %in% agg_level) %>%
filter(reference_period_desc %in% ref_period) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(asd_desc,
agg_level_desc, year, short_desc, class_desc, domain_desc, value_char =value_trim, unit_desc, commodity_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char)%>%
separate(short_desc, c("crop", 'info'), "- ")
}
ID<-sales(id_ops_raw_data)
US<-sales(us_ops_raw_data)
#It would be ideal if there was an automated sorting and processing of this data to get at values, also -- we used CDL areas to get at the value for 2010 (e.g. crops that aren't going to be quantified bc of an individual farmer)
write.csv(ID, file='IdahoSales_2010.csv')
write.csv(US, file='NationalSales_2010.csv')
unique(id_ops_raw_data$commodity_desc)
unique(ID$commodity_desc)
library(sp)
library(raster)
library(rgdal)
library(FedData)
library(cdlTools)
setwd("C:\Users\kendrakaiser\Dropbox\BSU\R\IM3\Data")
d01<-shapefile("d01.shp")
srb<-shapefile("SnakeRiverBasin.shp")
setwd("~/Dropbox/BSU/R/IM3/Data")
d01<-shapefile("d01.shp")
srb<-shapefile("SnakeRiverBasin.shp")
i=1
cdl <- raster(files2017[i])
cdlname<- paste("files", yr[i], sep="")
yr=c(2005)
cdlname<- paste("files", yr[i], sep="")
ptn=c("2005_..\\.tif$")
filist <- list.files(pattern = ptn[i])
filist
library(sp)
library(raster)
library(rgdal)
library(FedData)
library(cdlTools)
install.packages("devtools")
devtools::install_github("ropensci/FedData")
install.packages("devtools")
setwd("~/Dropbox/BSU/R/IM3/Data")
cdl012005<-getCDL(c("ID", "WY", "OR", "WA", "UT"), 2005, location = "./CDL") #2001:2017
library(sp)
library(raster)
library(rgdal)
library(FedData)
library(cdlTools)
install.packages(c("cdlTools", "FedData"))
library(sp)
library(raster)
library(rgdal)
library(FedData)
library(cdlTools)
library(sp)
library(raster)
library(rgdal)
library(FedData)
library(cdlTools)
cdl012005<-getCDL(c("ID", "WY", "OR", "WA", "UT"), 2005, location = "./CDL") #2001:2017
cdl10_srb<-crop(cdl012005, srb)
cdl <- raster(cdl012005[1])
setwd("~/Dropbox/BSU/R/IM3/Data/CDL")
files <- list.files(pattern = "\\.tif$")
cdlname=files[1]
cdl <- raster(cdlname)
cdl05_srb<-crop(cdl, srb)
cdl@extent
srb@proj4string
cdl@crs
categories<-c("AREA HARVESTED", "PRICE RECEIVED", "YIELD")
agg_level<-c("STATE", "NATIONAL", "AGRICULTURAL DISTRICT", "COUNTY")
ref_period<-c("YEAR", "MARKETING YEAR")
sales<- function(raw_data){
out <- raw_data %>%
#filter to specific data
filter(statisticcat_desc %in% categories)%>%
filter(agg_level_desc %in% agg_level) %>%
filter(reference_period_desc %in% ref_period) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(asd_desc,
agg_level_desc, year, short_desc, class_desc, domain_desc, value_char =value_trim, unit_desc, commodity_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char)%>%
separate(short_desc, c("crop", 'info'), "- ")
}
ID<-sales(id_ops_raw_data)
US<-sales(us_ops_raw_data)
#It would be ideal if there was an automated sorting and processing of this data to get at values, also -- we used CDL areas to get at the value for 2010 (e.g. crops that aren't going to be quantified bc of an individual farmer)
write.csv(ID, file='IdahoSales_2010.csv')
write.csv(US, file='NationalSales_2010.csv')
library(httr)
library(jsonlite)
library(tidycensus)
library(tidyverse)
library(purrr)
library(mapview)
library(dplyr)
ID<-sales(id_ops_raw_data)
US<-sales(us_ops_raw_data)
#It would be ideal if there was an automated sorting and processing of this data to get at values, also -- we used CDL areas to get at the value for 2010 (e.g. crops that aren't going to be quantified bc of an individual farmer)
write.csv(ID, file='IdahoSales_2010.csv')
write.csv(US, file='NationalSales_2010.csv')
unique(ID$crop)
unique(id_ops_raw_data$asd_desc)
setwd("~/Documents/GitRepos/IM3-BoiseState/ABMdev/Data/NASS_Data")
categories<-c("AREA HARVESTED", "PRICE RECEIVED", "YIELD")
agg_level<-c("STATE", "NATIONAL", "AGRICULTURAL DISTRICT", "COUNTY")
ref_period<-c("YEAR", "MARKETING YEAR")
gistrict<-c("SOUTHWEST", "EAST", "SOUTH CENTRAl")
sales<- function(raw_data){
out <- raw_data %>%
#filter to specific data
filter(statisticcat_desc %in% categories)%>%
filter(agg_level_desc %in% agg_level) %>%
filter(reference_period_desc %in% ref_period) %>%
filter(asd_desc %in% district) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(asd_desc,
agg_level_desc, year, short_desc, class_desc, domain_desc, value_char =value_trim, unit_desc, commodity_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char)%>%
separate(short_desc, c("crop", 'info'), "- ")
}
ID<-sales(id_ops_raw_data)
US<-sales(us_ops_raw_data)
#It would be ideal if there was an automated sorting and processing of this data to get at values, also -- we used CDL areas to get at the value for 2010 (e.g. crops that aren't going to be quantified bc of an individual farmer)
write.csv(ID, file='IdahoSales_2010.csv')
write.csv(US, file='NationalSales_2010.csv')
uniqe(ID$crop)
unique(ID$crop)
View(ID)
categories<-c("AREA HARVESTED", "PRICE RECEIVED", "YIELD")
agg_level<-c("STATE", "NATIONAL", "AGRICULTURAL DISTRICT", "COUNTY")
ref_period<-c("YEAR", "MARKETING YEAR")
district<-c("SOUTHWEST", "EAST", "SOUTH CENTRAl")
sales<- function(raw_data){
out <- raw_data %>%
#filter to specific data
filter(statisticcat_desc %in% categories)%>%
filter(agg_level_desc %in% agg_level) %>%
filter(reference_period_desc %in% ref_period) %>%
filter(asd_desc %in% district) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(asd_desc,
agg_level_desc, year, short_desc, class_desc, domain_desc, value_char =value_trim, unit_desc, commodity_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char)%>%
separate(short_desc, c("crop", 'info'), "- ")
}
ID<-sales(id_ops_raw_data)
US<-sales(us_ops_raw_data)
#It would be ideal if there was an automated sorting and processing of this data to get at values, also -- we used CDL areas to get at the value for 2010 (e.g. crops that aren't going to be quantified bc of an individual farmer)
write.csv(ID, file='IdahoSales_2010.csv')
write.csv(US, file='NationalSales_2010.csv')
unique(ID$crop)
View(ID)
View(ID)
View(ID)
View(ID)
View(ID)
my_year <- "2017"
###--------------------------------------#
# Download data and turn into dataframe
#####
# ID path string
path_id_ops <- paste0("api/api_GET/?key=", nass_key, "&sector_desc=", my_group_desc, "&year=", my_year, "&state_alpha=", "ID")
#unpack JSON object
raw_id_ops <- GET(url = nass_url, path = path_id_ops)
char_raw_id_ops<- rawToChar(raw_id_ops$content)
# check size of object
nchar(char_raw_id_ops)
#turn into list
list_raw_id_ops<- fromJSON(char_raw_id_ops)
# apply rbind to each row of the list and convert to a data frame
id_ops_raw_data <- pmap_dfr(list_raw_id_ops, rbind)
# US path string
path_us_ops <- paste0("api/api_GET/?key=", nass_key, "&sector_desc=", my_group_desc, "&year=", my_year, "&state_alpha=", "US")
#unpack JSON object
raw_us_ops <- GET(url = nass_url, path = path_us_ops)
char_raw_us_ops<- rawToChar(raw_us_ops$content)
# check size of object
nchar(char_raw_us_ops)
#turn into list
list_raw_us_ops<- fromJSON(char_raw_us_ops)
# apply rbind to each row of the list and convert to a data frame
us_ops_raw_data <- pmap_dfr(list_raw_us_ops, rbind)
###--------------------------------------#
# Subset Data based on highest value crops
#####
categories<-c("AREA HARVESTED", "PRICE RECEIVED", "YIELD")
agg_level<-c("STATE", "NATIONAL", "AGRICULTURAL DISTRICT", "COUNTY")
ref_period<-c("YEAR", "MARKETING YEAR")
district<-c("SOUTHWEST", "EAST", "SOUTH CENTRAl")
sales<- function(raw_data){
out <- raw_data %>%
#filter to specific data
filter(statisticcat_desc %in% categories)%>%
filter(agg_level_desc %in% agg_level) %>%
filter(reference_period_desc %in% ref_period) %>%
filter(asd_desc %in% district) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(asd_desc,
agg_level_desc, year, short_desc, class_desc, domain_desc, value_char =value_trim, unit_desc, commodity_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char)%>%
separate(short_desc, c("crop", 'info'), "- ")
}
ID<-sales(id_ops_raw_data)
View(ID)
View(ID)
unique(id_ops_raw_data$commodity_desc)
my_year <- "2005"
###--------------------------------------#
# Download data and turn into dataframe
#####
# ID path string
path_id_ops <- paste0("api/api_GET/?key=", nass_key, "&sector_desc=", my_group_desc, "&year=", my_year, "&state_alpha=", "ID")
#unpack JSON object
raw_id_ops <- GET(url = nass_url, path = path_id_ops)
char_raw_id_ops<- rawToChar(raw_id_ops$content)
# check size of object
nchar(char_raw_id_ops)
#turn into list
list_raw_id_ops<- fromJSON(char_raw_id_ops)
# apply rbind to each row of the list and convert to a data frame
id_ops_raw_data <- pmap_dfr(list_raw_id_ops, rbind)
unique(id_ops_raw_data$commodity_desc)
